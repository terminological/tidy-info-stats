---
title: "Untitled"
output: html_document
knit: (function(inputFile, encoding,...) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "~/Dropbox/featureSelection/tidyinfostats") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(infotheo)
library(tidyverse)
library(ggplot2)
library(devtools)
library(tidyinfostats)
#devtools::load_all("..")
set.seed(101)
theme_set(standardPrintOutput::defaultFigureLayout())
```

# Check entropy calculations working:

Using set of distributions:

```{r}
#devtools::load_all("..")
dists = list(
  normal = NormalDistribution$new(mean = 1, sd = 0.5),
  logNormal = LogNormalDistribution$new(mode = 1, sd = 0.5),
  uniform = UniformDistribution$new(min = 0,max = 2)
)

dists$normal$plot(-2,5)
```

```{r}

#devtools::load_all("..")

bins = 24
cutsDf = tibble(cut=seq(-1,3,length.out = bins-1))

out = NULL
method = c("MontgomerySmith", "Histogram","InfoTheo","Compression","Grassberger")
for (dist in dists) {
  thEnt = dist$theoreticalEntropy()
  for (i in c(1:10)) { #0)) {
    for (j in c(1:10,20,40,60,80,100)) { #,15,20,25,30,35,40,45,50,60,70,80,90,100)) {
      s = dist$sample(n=10*j)
      s = s %>% discretise(x, x_discrete, method="Manual", cutsDf=cutsDf)
      #s = s %>% discretise(x, x_discrete, method="ByRank")
      for (m in method) {
        estimate = calculateEntropy(s, groupVars=vars(x_discrete), method = m, infoTheoMethod = "emp", mm=FALSE) %>% pull(H)
        out = out %>% bind_rows(tibble(
          N = 10*j,
          method = m,
          C_x = length(unique(s$x_discrete)),
          dist = dist$label(),
          estimate = estimate,
          theoretical = thEnt
        ))
      }
    }
  }
}

# TODO: the theoretical mutual information is calculated using an integral which assumes equal bin width
# There is probably an adjustment that involves log(Number of non empty bins) and log(Number of bins) that 
# would adjust the estimated values to come into line with the theoretical.
# not sure this really works

out.summ = out %>% group_by(N,method,dist) %>% summarise(
  mean = mean(estimate),
  sd = sd(estimate),
  #th = max(log(7/theoretical))
  #th = max(log(20)-theoretical)
  #th = max(log(bins*theoretical))
  #th = max(log(bins)*theoretical)
  th = mean(log(C_x)),
  th.sd = sd(log(C_x)),
  th2 = max(theoretical)
) 

ggplot(out.summ, aes(x=N,colour=method,fill=method))+
  #geom_line(aes(y = th), colour="black",alpha=0.1)+
  #geom_ribbon((aes(ymin=th-1.96*th.sd, ymax=th+1.96*th.sd)),colour=NA,alpha=0.1)+
  geom_line(aes(y=mean))+
  geom_ribbon((aes(ymin=mean-1.96*sd, ymax=mean+1.96*sd)),colour=NA,alpha=0.2)+
  
  geom_hline(aes(yintercept = th2),colour="grey50")+
  geom_hline(yintercept=log(bins),colour="grey50")+
  facet_wrap(vars(dist))+scale_x_log10()
```



## Mutual information of continuous versus discrete disctirbutions

Assume a continuous distribution

```{r}
hb = ConditionalDistribution$new()
hb$withDistribution(0.75, LogNormalDistribution$new(mode=12,sd=2), "asymptomatic")
hb$withDistribution(0.25, LogNormalDistribution$new(mode=8,sd=3), "tired")
hb$withDistribution(0.25, LogNormalDistribution$new(mode=4,sd=5), "unwell")
hb$plot(0,20)

```

And another

```{r}
k = ConditionalDistribution$new()
k$withDistribution(0.125, NormalDistribution$new(mean=1,sd=0.5), "unwell")
k$withDistribution(0.75, NormalDistribution$new(mean=2,sd=1), "asymptomatic")
k$withDistribution(0.125, NormalDistribution$new(mean=8,sd=3), "tired")
```

We can use the properties of the underlying distributions to calculate theoretical values for the Mean, Variance and Mutual Information

```{r}
tibble(measure = c("Mean","Variance","Mutual Information"),
hb = c(hb$theoreticalMean(),hb$theoreticalVariance(), hb$theoreticalMI()),
k = c(k$theoreticalMean(),k$theoreticalVariance(),k$theoreticalMI()))
```

We can also generate test data by sampling from the underlying distributions

```{r}
set.seed(101)
testData = k$sample(500) %>% mutate(test="k") %>% bind_rows(
  hb$sample(1000) %>% mutate(test="hb")) %>% rename(outcome=y, value=x)

testData = testData[sample(nrow(testData)),]

ggplot(testData, aes(fill=outcome, x=value))+geom_histogram(position="dodge", bins=50)+facet_grid(cols=vars(test))
```

And use this sample to estimate the MI using a range of methods.

TODO: Explain the methods.

```{r}
#devtools::load_all("..")
#debug(calculateDiscreteDiscreteMI)
#undebug(calculateDiscreteContinuousMI_KNN)
#debug(calculateDiscreteDiscreteMI_Entropy)
results = NULL
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "KWindow"))
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "SGolay"))
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByRank"))
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByValue", discreteMethod="Entropy", entropyMethod="InfoTheo"))
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByValue", discreteMethod="Histogram"))
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByValue", discreteMethod="MontgomerySmith"))
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByValue", discreteMethod="Entropy", entropyMethod="Grassberger"))
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "KNN", useKWindow=FALSE))
set.seed(101)
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "Compression"))
set.seed(101)
results = results %>% rbind(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "DiscretiseByRank", binStrategy = linearBySize(8,4,256), discreteMethod="Entropy", entropyMethod="Compression"))
results = results %>% rbind(tibble(test=c("hb","k"),I = c(hb$theoreticalMI(),k$theoreticalMI()), I_sd=NA,method=rep("theoretical",2)))

results %>% group_by(test) %>% standardPrintOutput::mergeCells()

# left_join(testData %>% group_by(test) %>% tidyinfostats::calculateDiscreteContinuousMI(outcome, value, method = "Entropy") %>% rename(I.emp=I), by="test") %>%
```


